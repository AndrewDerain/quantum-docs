<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="QuantumWriter">
  <meta name="keywords" content="Don’t add or edit keywords without consulting your SEO champ.">
  <title>Intent and product brand in a unique string of 43-59 chars including spaces | Microsoft Docs</title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header>
<h1 class="title">Intent and product brand in a unique string of 43-59 chars including spaces | Microsoft Docs</h1>
<p class="author">QuantumWriter</p>
</header>
<h1 id="vectors-and-matrices">Vectors and Matrices</h1>
<p>Some familiarity with vectors and matrices is essential to understand quantum computing. We provide a brief introduction below and interested readers are recommended to read a standard reference on linear algebra such as [Strang, G. (1993). Introduction to linear algebra (Vol. 3). Wellesley, MA: Wellesley-Cambridge Press] or an online reference such as http://joshua.smcvt.edu/linearalgebra/.</p>
<p>A column vector (or simply vector) <span class="math inline">\(v\)</span> of dimension (or size) <span class="math inline">\(n\)</span> is a collection of <span class="math inline">\(n\)</span> complex numbers <span class="math inline">\((v_1,v_2, \ldots,v_n)\)</span> arranged as a column: <span class="math display">\[v =\begin{bmatrix}
v_1\\
v_2\\
\vdots\\
v_n
\end{bmatrix}.\]</span> The norm of a vector <span class="math inline">\(v\)</span> is defined as <span class="math inline">\(\sqrt{\sum_i |v_i|^2}\)</span>. A vector is said to be of unit norm (or alternatively it is called a unit vector) if its norm is <span class="math inline">\(1\)</span>. The adjoint of a vector <span class="math inline">\(v\)</span> is denoted <span class="math inline">\(v^\dagger\)</span> and is defined to be the following row vector <span class="math display">\[
\begin{bmatrix}x_1 \\ \vdots \\ x_n \end{bmatrix}^\dagger = \begin{bmatrix}x_1^* &amp; \cdots &amp; x_n^* \end{bmatrix}.
\]</span></p>
<p>The most common way to multiply two vectors together is through the inner product, otherwise known as a dot product. The inner product gives the projection of one vector onto another and is invaluable in describing how to express one vector as a sum of other simpler vectors. The inner product between <span class="math inline">\(\phi\)</span> and <span class="math inline">\(\psi\)</span>, denoted <span class="math inline">\(\left\langle \phi, \psi\right\rangle\)</span> is defined as <span class="math display">\[
\left\langle \begin{bmatrix}\phi_1 \\\vdots\\ \phi_{n} \end{bmatrix},\begin{bmatrix}\psi_1 \\\vdots\\ \psi_{n} \end{bmatrix} \right\rangle = \phi^\dagger \psi=\phi_1^*\psi_1 + \cdots + \phi_n^* \psi_n.
\]</span> This notation also allows the norm of a vector <span class="math inline">\(v\)</span> to be written as <span class="math inline">\(\sqrt{\langle v, v\rangle}\)</span>.</p>
<p>We can multiply a vector with a number <span class="math inline">\(c\)</span> to form a new vector whose entries are multiplied by <span class="math inline">\(c\)</span>. We can also add two vectors <span class="math inline">\(u\)</span> and <span class="math inline">\(v\)</span> to form a new vector whose entries are the sum of the entries of <span class="math inline">\(u\)</span> and <span class="math inline">\(v\)</span>. These operations are depicted below: <span class="math display">\[\mathrm{If}~u =\begin{bmatrix}
u_1\\
u_2\\
\vdots\\
u_n
\end{bmatrix}~\mathrm{and}~
v =\begin{bmatrix}
    v_1\\
    v_2\\
    \vdots\\
    v_n
\end{bmatrix},~\mathrm{then}~
au+bv =\begin{bmatrix}
au_1+bv_1\\
au_2+bv_2\\
\vdots\\
au_n+bv_n
\end{bmatrix}.
\]</span></p>
<p>A matrix of size <span class="math inline">\(m \times n\)</span> is a collection of <span class="math inline">\(mn\)</span> complex numbers arranged in <span class="math inline">\(m\)</span> rows and <span class="math inline">\(n\)</span> columns as shown below: <span class="math display">\[M = 
\begin{bmatrix}
M_{11} ~~ M_{12} ~~ \cdots ~~ M_{1n}\\
M_{21} ~~ M_{22} ~~ \cdots ~~ M_{2n}\\
\ddots\\
M_{m1} ~~ M_{m2} ~~ \cdots ~~ M_{mn}\\
\end{bmatrix}\]</span></p>
<p>Note that a vector of dimension <span class="math inline">\(n\)</span> is simply a matrix of size <span class="math inline">\(n \times 1\)</span>. Just like with vectors, we can multiply a matrix with a number <span class="math inline">\(c\)</span> to obtain a new matrix where every entry is multiplied with <span class="math inline">\(c\)</span>, and we can add two matrices of the same size to produce a new matrix whose entries are the sum of the respective entries of the two matrices.</p>
<p>We can also multiply two matrices <span class="math inline">\(M\)</span> of dimension <span class="math inline">\(m\times n\)</span> and <span class="math inline">\(N\)</span> of dimension <span class="math inline">\(n \times p\)</span> to get a new matrix <span class="math inline">\(P\)</span> of dimension <span class="math inline">\(m \times p\)</span> as follows: <span class="math display">\[
\begin{bmatrix}
    M_{11} ~~ M_{12} ~~ \cdots ~~ M_{1n}\\
    M_{21} ~~ M_{22} ~~ \cdots ~~ M_{2n}\\
    \ddots\\
    M_{m1} ~~ M_{m2} ~~ \cdots ~~ M_{mn}\\
\end{bmatrix}\times
\begin{bmatrix}
N_{11} ~~ N_{12} ~~ \cdots ~~ N_{1p}\\
N_{21} ~~ N_{22} ~~ \cdots ~~ N_{2p}\\
\ddots\\
N_{n1} ~~ N_{n2} ~~ \cdots ~~ N_{np}\\
\end{bmatrix} =
\begin{bmatrix}
P_{11} ~~ P_{12} ~~ \cdots ~~ P_{1p}\\
P_{21} ~~ P_{22} ~~ \cdots ~~ P_{2p}\\
\ddots\\
P_{m1} ~~ P_{m2} ~~ \cdots ~~ P_{mp}\\
\end{bmatrix},
\]</span> where the entry <span class="math inline">\(P_{ik} = \sum_j M_{ij}N_{jk}\)</span>. For example, the entry <span class="math inline">\(P_{11}\)</span> is the inner product of the first row of <span class="math inline">\(M\)</span> with the first column of <span class="math inline">\(N\)</span>. Note that since a vector is simply a special case of a matrix, this definition extends to matrix–vector multiplication.</p>
<p>All the matrices we consider will either be square matrices, where the number of rows and columns are equal, or vectors, which corresponds to only <span class="math inline">\(1\)</span> column. One special square matrix is the identity matrix, denoted <span class="math inline">\(\openone\)</span>, which has all its diagonal elements equal to <span class="math inline">\(1\)</span> and the remaining elements equal to <span class="math inline">\(0\)</span>: <span class="math display">\[\openone=\begin{bmatrix}
1 ~~ 0 ~~ \cdots ~~ 0\\
0 ~~ 1 ~~ \cdots ~~ 0\\
~~ \ddots\\
0 ~~ 0 ~~ \cdots ~~ 1\\
\end{bmatrix}.\]</span> For a square matrix <span class="math inline">\(A\)</span>, we say a matrix <span class="math inline">\(B\)</span> is its inverse if <span class="math inline">\(AB = \openone\)</span>. The inverse of a matrix need not exist, but when it exists it is unique and we denote it <span class="math inline">\(A^{-1}\)</span>. For any matrix <span class="math inline">\(M\)</span>, the adjoint or conjugate transpose of <span class="math inline">\(M\)</span>, is a matrix <span class="math inline">\(N\)</span> such that <span class="math inline">\(N_{ij} = M^*_{ji}\)</span>. The adjoint of <span class="math inline">\(M\)</span> is usually denoted <span class="math inline">\(M^\dagger\)</span>. We say a matrix <span class="math inline">\(U\)</span> is unitary if <span class="math inline">\(UU^\dagger = \openone\)</span> or equivalently, <span class="math inline">\(U^{-1} = U^\dagger\)</span>. Perhaps the most important property of unitary matrices is that they preserve the norm of a vector. This happens because <span class="math inline">\(\langle v,v \rangle=v^\dagger v = v^\dagger U^{-1} U v = \langle U v, U v\rangle\)</span>.</p>
<p>Finally, the tensor product (or Kronecker product) of two matrices <span class="math inline">\(M\)</span> of size <span class="math inline">\(m\times n\)</span> and <span class="math inline">\(N\)</span> of size <span class="math inline">\(p \times q\)</span> is a larger matrix <span class="math inline">\(P=M\otimes N\)</span> of size <span class="math inline">\(mp \times nq\)</span>, and is obtained from <span class="math inline">\(M\)</span> and <span class="math inline">\(N\)</span> as follows: <span class="math display">\[
M \otimes N = 
\begin{bmatrix}
M_{11} ~~ \cdots ~~ M_{1n}\\
\ddots\\
M_{m1}  ~~ \cdots ~~ M_{mn}\\
\end{bmatrix}\times
\begin{bmatrix}
N_{11}  ~~ \cdots ~~ N_{1q}\\
\ddots\\
N_{p1} ~~ \cdots ~~ N_{pq}\\
\end{bmatrix}
=
\begin{bmatrix}
M_{11} \begin{bmatrix} N_{11}  ~~ \cdots ~~ N_{1q}\\ \ddots\\ N_{p1} ~~ \cdots ~~ N_{pq}\\ \end{bmatrix}~~ \cdots ~~ 
M_{1n} \begin{bmatrix} N_{11}  ~~ \cdots ~~ N_{1q}\\ \ddots\\ N_{p1} ~~ \cdots ~~ N_{pq}\\ \end{bmatrix}\\
\ddots\\
M_{m1} \begin{bmatrix} N_{11}  ~~ \cdots ~~ N_{1q}\\ \ddots\\ N_{p1} ~~ \cdots ~~ N_{pq}\\ \end{bmatrix}~~ \cdots ~~ 
M_{mn} \begin{bmatrix} N_{11}  ~~ \cdots ~~ N_{1q}\\ \ddots\\ N_{p1} ~~ \cdots ~~ N_{pq}\\ \end{bmatrix}\\
\end{bmatrix}.
\]</span> This is better demonstrated using some examples: <span class="math display">\[
\begin{bmatrix} a \\ b  \end{bmatrix} \otimes \begin{bmatrix} c \\ d \\ e\end{bmatrix} 
=\begin{bmatrix} a \begin{bmatrix} c \\ d \\ e\end{bmatrix}  \\[1.5em] b \begin{bmatrix} c \\ d \\ e\end{bmatrix}  \end{bmatrix}
= \begin{bmatrix} a c \\ a d \\a e \\ b c \\ b d \\ be\end{bmatrix}, \mathrm{~~and} \quad 
\begin{bmatrix}
a\ b\\c\ d
\end{bmatrix}
\otimes 
\begin{bmatrix}
e\ f\\g\ h
\end{bmatrix}
=
\begin{bmatrix}
a\begin{bmatrix}
e\ f\\g\ h
\end{bmatrix}
b\begin{bmatrix}
e\ f\\g\ h
\end{bmatrix}
\\[1em]
c\begin{bmatrix}
e\ f\\g\ h
\end{bmatrix}
d\begin{bmatrix}
e\ f\\g\ h
\end{bmatrix}
\end{bmatrix}
=
\begin{bmatrix}
ae\ af\ be\ bf\\
ag\ ah\ bg\ bh\\
ce\ cf\ de\ df\\
cg\ ch\ dg\ dh
\end{bmatrix}.
\]</span></p>
<p>A final notation surrounding tensor products that is useful is that, for any vector <span class="math inline">\(v\)</span> or matrix <span class="math inline">\(M\)</span>, <span class="math inline">\(v^{\otimes n}\)</span> or <span class="math inline">\(M^{\otimes n}\)</span> is short hand for an <span class="math inline">\(n\)</span>–fold repeated tensor product. For example <span class="math display">\[
\begin{bmatrix} 1 \\ 0 \end{bmatrix}^{\otimes 1} = \begin{bmatrix} 1 \\ 0 \end{bmatrix}\qquad \begin{bmatrix} 1 \\ 0 \end{bmatrix}^{\otimes 2} = \begin{bmatrix} 1 \\ 0\\0\\0 \end{bmatrix}\qquad X^{\otimes 2}= \begin{bmatrix} 0 &amp;0&amp;0&amp;1 \\ 0 &amp;0&amp;1&amp;0 \\0 &amp;1&amp;0&amp;0\\1 &amp;0&amp;0&amp;0\end{bmatrix}.
\]</span></p>
<h1 id="matrix-exponentials">Matrix Exponentials</h1>
<p>A matrix exponential can also be defined in exact analogy to the exponential function. The matrix exponential of a matrix <span class="math inline">\(A\)</span> can be expressed as <span class="math display">\[
e^A=\openone + A + \frac{A^2}{2!}+\frac{A^3}{3!}+\cdots.
\]</span> This is important to us because quantum mechanical time evolution is described by a unitary matrix of the form <span class="math inline">\(e^{iB}\)</span> for Hermitian matrix <span class="math inline">\(B\)</span>. For this reason, performing matrix exponentials is a fundamental part of quantum computing and as such Q# has intrinsic routines for describing these operations. There are many ways in practice to compute a matrix exponential on a classical computer, and in general numerically approximating such an exponential it is fraught with peril. See [Moler, Cleve, and Charles Van Loan. “Nineteen dubious ways to compute the exponential of a matrix.” SIAM review 20.4 (1978): 801-836] for more information about the challenges involved.</p>
<p>The easiest way to understand how to compute the exponential of a matrix is through the eigenvalues and eigenvectors of that matrix. Specifically, the eigenvalue decomposition says that for every <span class="math inline">\(N\times N\)</span> matrix <span class="math inline">\(A\)</span> there exists a unitary matrix <span class="math inline">\(U\)</span> and a diagonal matrix <span class="math inline">\(D\)</span> such that <span class="math inline">\(A=U^\dagger D U\)</span>. Because of the properties of unitarity we have that <span class="math inline">\(A^2 = U^\dagger D^2 U\)</span> and similarly for any power <span class="math inline">\(p\)</span> <span class="math inline">\(A^p = U^\dagger D^p U\)</span>. If we substitute this into the operator definition of the operator exponential we obtain <span class="math display">\[
e^A= U^\dagger \left(\openone +D +\frac{D^2}{2!}+\cdots \right)U= U^\dagger \begin{bmatrix}\exp(D_{11}) &amp; 0 &amp;\cdots &amp;0\\ 0 &amp; \exp(D_{22})&amp;\cdots&amp; 0\\ \vdots &amp;\vdots &amp;\ddots &amp;\vdots\\ 0&amp;0&amp;\cdots&amp;\exp(D_{NN}) \end{bmatrix} U.
\]</span> In other words, if you transform to the eigenbasis of the matrix <span class="math inline">\(A\)</span> then computing the matrix exponential is equivalent to computing the ordinary exponential of the eigenvalues of the matrix. As many of the operations in quantum computing involve performing matrix exponentials, this trick of transforming into the eigenbasis of a matrix to simplify performing the operator exponential appears frequently and is the basis behind many quantum algorithms such as Trotter-Suzuki based quantum simulation methods.</p>
<p>Another useful property is if <span class="math inline">\(B\)</span> is both unitary and Hermitian, ie <span class="math inline">\(B^2=\openone\)</span>, then it can be seen by applying this rule to the above expansion of the operator exponential and grouping the <span class="math inline">\(\openone\)</span> and the <span class="math inline">\(B\)</span> terms that for any real valued <span class="math inline">\(x\)</span> <span class="math display">\[e^{iBx}=\cos(x)\openone + iB\sin(x).\]</span> This trick is especially useful because it allows us to reason about the actions that matrix exponentials have, even if the dimension of <span class="math inline">\(B\)</span> is exponentially large, for the special case when <span class="math inline">\(B\)</span> is both unitary and Hermitian.</p>
</body>
</html>
